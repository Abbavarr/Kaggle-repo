{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-01-17T13:59:04.726862Z",
     "iopub.status.busy": "2021-01-17T13:59:04.726090Z",
     "iopub.status.idle": "2021-01-17T13:59:07.704016Z",
     "shell.execute_reply": "2021-01-17T13:59:07.703354Z"
    },
    "papermill": {
     "duration": 3.018833,
     "end_time": "2021-01-17T13:59:07.704168",
     "exception": false,
     "start_time": "2021-01-17T13:59:04.685335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import gensim\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models.wrappers import FastText\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# encoder_path = './encoder_hs1000es300p1100.pt'\n",
    "# decoder_path = './decoder_hs1000es300p1100.pt'\n",
    "# model_path = './model_hs1000es300p1100.pt'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T13:59:07.760008Z",
     "iopub.status.busy": "2021-01-17T13:59:07.759398Z",
     "iopub.status.idle": "2021-01-17T13:59:07.930732Z",
     "shell.execute_reply": "2021-01-17T13:59:07.930101Z"
    },
    "papermill": {
     "duration": 0.201164,
     "end_time": "2021-01-17T13:59:07.930856",
     "exception": false,
     "start_time": "2021-01-17T13:59:07.729692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n",
    "train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\n",
    "train = train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T13:59:07.988964Z",
     "iopub.status.busy": "2021-01-17T13:59:07.987977Z",
     "iopub.status.idle": "2021-01-17T13:59:08.001742Z",
     "shell.execute_reply": "2021-01-17T13:59:08.001105Z"
    },
    "papermill": {
     "duration": 0.046403,
     "end_time": "2021-01-17T13:59:08.001850",
     "exception": false,
     "start_time": "2021-01-17T13:59:07.955447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_pairs = train[['text','selected_text']]\n",
    "train_pairs = train_pairs.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T13:59:08.063027Z",
     "iopub.status.busy": "2021-01-17T13:59:08.062263Z",
     "iopub.status.idle": "2021-01-17T13:59:08.065267Z",
     "shell.execute_reply": "2021-01-17T13:59:08.064801Z"
    },
    "papermill": {
     "duration": 0.03741,
     "end_time": "2021-01-17T13:59:08.065365",
     "exception": false,
     "start_time": "2021-01-17T13:59:08.027955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_df = pd.concat([train['text'],test['text']],axis=0,ignore_index=True)\n",
    "vocab_list = vocab_df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T13:59:08.121733Z",
     "iopub.status.busy": "2021-01-17T13:59:08.121004Z",
     "iopub.status.idle": "2021-01-17T13:59:08.123968Z",
     "shell.execute_reply": "2021-01-17T13:59:08.123479Z"
    },
    "papermill": {
     "duration": 0.033534,
     "end_time": "2021-01-17T13:59:08.124064",
     "exception": false,
     "start_time": "2021-01-17T13:59:08.090530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = test['text']\n",
    "test_list = test_df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T13:59:08.191999Z",
     "iopub.status.busy": "2021-01-17T13:59:08.191273Z",
     "iopub.status.idle": "2021-01-17T13:59:12.669862Z",
     "shell.execute_reply": "2021-01-17T13:59:12.669255Z"
    },
    "papermill": {
     "duration": 4.51864,
     "end_time": "2021-01-17T13:59:12.669979",
     "exception": false,
     "start_time": "2021-01-17T13:59:08.151339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 34\n",
    "\n",
    "sos_tensor = torch.tensor(SOS_token, dtype=torch.long, device=device)\n",
    "eos_tensor = torch.tensor(EOS_token, dtype=torch.long, device=device)\n",
    "\n",
    "# class Lang:\n",
    "#     def __init__(self, name):\n",
    "#         self.name = name\n",
    "#         self.word2index = {}\n",
    "#         self.word2count = {}\n",
    "#         self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "#         self.n_words = 2  # Count SOS and EOS\n",
    "        \n",
    "#     def addSentence(self, sentence):\n",
    "#         for word in sentence.split(' '):\n",
    "#             self.addWord(word)\n",
    "\n",
    "#     def addWord(self, word):\n",
    "#         if word not in self.word2index:\n",
    "#             self.word2index[word] = self.n_words\n",
    "#             self.word2count[word] = 1\n",
    "#             self.index2word[self.n_words] = word\n",
    "#             self.n_words += 1\n",
    "#         else:\n",
    "#             self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T13:59:12.727967Z",
     "iopub.status.busy": "2021-01-17T13:59:12.727158Z",
     "iopub.status.idle": "2021-01-17T13:59:12.730156Z",
     "shell.execute_reply": "2021-01-17T13:59:12.729596Z"
    },
    "papermill": {
     "duration": 0.034346,
     "end_time": "2021-01-17T13:59:12.730248",
     "exception": false,
     "start_time": "2021-01-17T13:59:12.695902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "#     s = re.sub(r'https?:\\/\\/.\\S+', \"\", s)\n",
    "#     s = re.sub(r'[-*0123456789¿½()\"]', ' ', s)\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "#     s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "#     s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "#     s = \" \".join([asd for asd in re.split(\"([A-Z][a-z]+[^A-Z]*)\",s) if asd])\n",
    "    s = s.strip()\n",
    "    s = s.split()\n",
    "    return \" \".join(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T13:59:12.793580Z",
     "iopub.status.busy": "2021-01-17T13:59:12.792775Z",
     "iopub.status.idle": "2021-01-17T13:59:14.794387Z",
     "shell.execute_reply": "2021-01-17T13:59:14.793799Z"
    },
    "papermill": {
     "duration": 2.039646,
     "end_time": "2021-01-17T13:59:14.794529",
     "exception": false,
     "start_time": "2021-01-17T13:59:12.754883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_list = [normalizeString(l) for l in vocab_list]\n",
    "test_list = [normalizeString(l) for l in test_list]\n",
    "train_pairs = [[normalizeString(s) for s in l] for l in train_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T13:59:14.848578Z",
     "iopub.status.busy": "2021-01-17T13:59:14.847791Z",
     "iopub.status.idle": "2021-01-17T13:59:14.850164Z",
     "shell.execute_reply": "2021-01-17T13:59:14.850756Z"
    },
    "papermill": {
     "duration": 0.031037,
     "end_time": "2021-01-17T13:59:14.850875",
     "exception": false,
     "start_time": "2021-01-17T13:59:14.819838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def readLangs(lang:str):\n",
    "#     pairs = train_pairs \n",
    "#     vocab_lang = Lang(lang)\n",
    "#     return vocab_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T13:59:14.904554Z",
     "iopub.status.busy": "2021-01-17T13:59:14.903807Z",
     "iopub.status.idle": "2021-01-17T13:59:14.906881Z",
     "shell.execute_reply": "2021-01-17T13:59:14.906279Z"
    },
    "papermill": {
     "duration": 0.031194,
     "end_time": "2021-01-17T13:59:14.906973",
     "exception": false,
     "start_time": "2021-01-17T13:59:14.875779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def prepareData(lang:str):\n",
    "#     vocab_lang, pairs = readLangs(lang)\n",
    "    \n",
    "#     print(\"Read %s sentence pairs\" % len(pairs))\n",
    "#     print(\"Counting words...\")\n",
    "#     for sentence in vocab_list:\n",
    "#         vocab_lang.addSentence(sentence)\n",
    "#     for sentence_pair in pairs:\n",
    "#         for sentence1 in sentence_pair:\n",
    "#             vocab_lang.addSentence(sentence1)\n",
    "        \n",
    "#     print(\"Counted words:\")\n",
    "#     print(vocab_lang.name, vocab_lang.n_words)\n",
    "#     return vocab_lang, pairs\n",
    "\n",
    "\n",
    "# vocab,  pairs = prepareData('vocab')\n",
    "\n",
    "# print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T13:59:14.966353Z",
     "iopub.status.busy": "2021-01-17T13:59:14.964408Z",
     "iopub.status.idle": "2021-01-17T13:59:14.969151Z",
     "shell.execute_reply": "2021-01-17T13:59:14.968685Z"
    },
    "papermill": {
     "duration": 0.03739,
     "end_time": "2021-01-17T13:59:14.969242",
     "exception": false,
     "start_time": "2021-01-17T13:59:14.931852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, dropout_odds=0.3):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout_odds)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.embedding.weight.requires_grad = True\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, bidirectional=True)\n",
    "        self.fc_hidden = nn.Linear(hidden_size*2, hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x)).unsqueeze(1)\n",
    "        output, hidden = self.gru(embedded)\n",
    "        hidden = self.fc_hidden(torch.cat((hidden[0:1],hidden[1:2]),dim=2))\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T13:59:15.024186Z",
     "iopub.status.busy": "2021-01-17T13:59:15.023360Z",
     "iopub.status.idle": "2021-01-17T13:59:15.026175Z",
     "shell.execute_reply": "2021-01-17T13:59:15.025694Z"
    },
    "papermill": {
     "duration": 0.031998,
     "end_time": "2021-01-17T13:59:15.026273",
     "exception": false,
     "start_time": "2021-01-17T13:59:14.994275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a,b,c = tensorFromPair(train_pairs[0][0])\n",
    "# hidden_size = 256\n",
    "# embedding_size=300\n",
    "# vocab_size = 35\n",
    "# dropout_odds = 0.3\n",
    "# encoder = Encoder(vocab_size, embedding_size, hidden_size).to(device)\n",
    "# encoder_outputs,hidden = encoder(a[0])\n",
    "# # dropout = nn.Dropout(dropout_odds).to(device)\n",
    "# # embedding = nn.Embedding(35, embedding_size).to(device)\n",
    "# # gru = nn.GRU(embedding_size, hidden_size, bidirectional=True).to(device)\n",
    "# # fc_hidden = nn.Linear(hidden_size*2, hidden_size).to(device)\n",
    "# # x = training_pairs[0][0]\n",
    "\n",
    "# # zxc = embedding(x)\n",
    "# # embedded = dropout(embedding(x)).unsqueeze(1)\n",
    "# # output, hidden = gru(embedded)\n",
    "# # hidden = fc_hidden(torch.cat((hidden[0:1],hidden[1:2]),dim=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T13:59:15.082112Z",
     "iopub.status.busy": "2021-01-17T13:59:15.081287Z",
     "iopub.status.idle": "2021-01-17T13:59:15.084050Z",
     "shell.execute_reply": "2021-01-17T13:59:15.083558Z"
    },
    "papermill": {
     "duration": 0.032403,
     "end_time": "2021-01-17T13:59:15.084146",
     "exception": false,
     "start_time": "2021-01-17T13:59:15.051743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# embedding = nn.Embedding(vocab_size, embedding_size).to(device)\n",
    "# energy = nn.Linear(hidden_size*3, 1).to(device)\n",
    "# gru = nn.GRU(hidden_size*2+embedding_size, hidden_size).to(device)\n",
    "# out = nn.Linear(hidden_size, vocab_size).to(device)\n",
    "# relu = nn.ReLU().to(device)\n",
    "# softmax_en = nn.Softmax(dim=0).to(device)\n",
    "# x = sos_tensor\n",
    "# target = a[1]\n",
    "# target_len = target.shape[0]\n",
    "# decoder_outputs = torch.zeros(target_len, 35).to(device)\n",
    "# eos_loss_tensor = torch.tensor([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,100]).to(device)\n",
    "# teacher_force_ratio = 0.5\n",
    "\n",
    "# embedded = embedding(x).unsqueeze(0).unsqueeze(0)\n",
    "# sequence_length = encoder_outputs.shape[0]\n",
    "# h_reshaped = hidden[0].repeat(sequence_length, 1,1)\n",
    "# energy_ = relu(energy(torch.cat((h_reshaped, encoder_outputs), dim=2)))\n",
    "# attention = softmax_en(energy_)\n",
    "# context_vector = torch.einsum(\"snk,snl->knl\", attention, encoder_outputs)\n",
    "# rnn_input = torch.cat((context_vector, embedded),dim=2)\n",
    "# output, hidden = gru(rnn_input, hidden)\n",
    "# output = relu(out(hidden)).squeeze(0)\n",
    "# if output.argmax() not in target[:-1]:\n",
    "#     output = torch.ger(output.squeeze(0),eos_loss_tensor.float())[1]\n",
    "# decoder_outputs[1] = output\n",
    "# best_guess = output.argmax()\n",
    "# x = target[1] if random.random() < teacher_force_ratio else best_guess\n",
    "# print(output)\n",
    "# print(output.argmax())\n",
    "# print(hidden.shape)\n",
    "# print(outVector2wordList(output,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T13:59:15.148447Z",
     "iopub.status.busy": "2021-01-17T13:59:15.147693Z",
     "iopub.status.idle": "2021-01-17T13:59:15.150548Z",
     "shell.execute_reply": "2021-01-17T13:59:15.150082Z"
    },
    "papermill": {
     "duration": 0.041208,
     "end_time": "2021-01-17T13:59:15.150643",
     "exception": false,
     "start_time": "2021-01-17T13:59:15.109435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, dropout_odds=0.3):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout_odds)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.embedding.weight.requires_grad = True\n",
    "        self.energy = nn.Linear(hidden_size*3, 1)\n",
    "        self.gru = nn.GRU(hidden_size*2+embedding_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, 35)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax_en = nn.Softmax(dim=0)\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden, encoder_outputs):\n",
    "        embedded = self.dropout(self.embedding(x)).unsqueeze(0).unsqueeze(0)\n",
    "        sequence_length = encoder_outputs.shape[0]\n",
    "        h_reshaped = hidden[0].repeat(sequence_length, 1,1)\n",
    "        energy_ = self.relu(self.energy(torch.cat((h_reshaped, encoder_outputs), dim=2)))\n",
    "        attention = self.softmax_en(energy_)\n",
    "        context_vector = torch.einsum(\"snk,snl->knl\", attention, encoder_outputs)\n",
    "        rnn_input = torch.cat((context_vector, embedded),dim=2)\n",
    "        output, hidden = self.gru(rnn_input, hidden)\n",
    "        \n",
    "        output = self.relu(self.out(hidden)).squeeze(0)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T13:59:15.212125Z",
     "iopub.status.busy": "2021-01-17T13:59:15.211359Z",
     "iopub.status.idle": "2021-01-17T13:59:15.213701Z",
     "shell.execute_reply": "2021-01-17T13:59:15.214267Z"
    },
    "papermill": {
     "duration": 0.037918,
     "end_time": "2021-01-17T13:59:15.214375",
     "exception": false,
     "start_time": "2021-01-17T13:59:15.176457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Seq2seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, source, target, teacher_force_ratio=0.5, output_size = 35):\n",
    "        sourse_len = source.shape[0]\n",
    "        target_len = target.shape[0]\n",
    "#         eos_loss_tensor = torch.tensor([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,100]).to(device)\n",
    "#         target_vocab_size = vocab.n_words\n",
    "#         hidden = torch.zeros(hiden_size)\n",
    "#         encoder_outputs =  torch.zeros(sourse_len,1, hidden_size*2).to(device)\n",
    "        decoder_outputs = torch.zeros(target_len, output_size).to(device)\n",
    "#         attention_outputs = torch.zeros(target_len,sourse_len).to(device)\n",
    "#         for ei in range(sourse_len):\n",
    "        encoder_states, hidden = self.encoder(source)\n",
    "#             encoder_outputs[ei] = encoder_states\n",
    "        x = sos_tensor\n",
    "\n",
    "        for t in range(target_len):\n",
    "#             output, hidden, _ = self.decoder(x, hidden, encoder_outputs)\n",
    "            output, hidden = self.decoder(x, hidden, encoder_states)\n",
    "            decoder_outputs[t] = output\n",
    "            best_guess = output.argmax()\n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "            \n",
    "\n",
    "        return decoder_outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T13:59:15.269205Z",
     "iopub.status.busy": "2021-01-17T13:59:15.268429Z",
     "iopub.status.idle": "2021-01-17T13:59:15.270998Z",
     "shell.execute_reply": "2021-01-17T13:59:15.271545Z"
    },
    "papermill": {
     "duration": 0.03182,
     "end_time": "2021-01-17T13:59:15.271653",
     "exception": false,
     "start_time": "2021-01-17T13:59:15.239833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def indexFromSentence(sentence):\n",
    "#     return [vocab.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "# def tensorFromIndex(sentance):\n",
    "#     indexes = indexFromSentence(sentance)\n",
    "#     indexes.append(EOS_token)\n",
    "#     return torch.tensor(indexes, dtype=torch.long, device=device)   \n",
    "    \n",
    "# def tensorsFromPair(pair):\n",
    "#     input_tensor = tensorFromIndex(pair[0])\n",
    "#     target_tensor = tensorFromIndex(pair[1])\n",
    "#     return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T13:59:15.336670Z",
     "iopub.status.busy": "2021-01-17T13:59:15.331788Z",
     "iopub.status.idle": "2021-01-17T13:59:15.339421Z",
     "shell.execute_reply": "2021-01-17T13:59:15.338976Z"
    },
    "papermill": {
     "duration": 0.041756,
     "end_time": "2021-01-17T13:59:15.339532",
     "exception": false,
     "start_time": "2021-01-17T13:59:15.297776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def indexFromInput(sentence):\n",
    "    i=1\n",
    "    word2idx_voc = {'SOS':0,'EOS':34}\n",
    "    idx2word_voc = {0:'SOS',34:'EOS'}\n",
    "    idx_list = []\n",
    "    for word in sentence.split(' '):\n",
    "        if word not in word2idx_voc:\n",
    "            word2idx_voc[word] = i\n",
    "            idx2word_voc[i] = word\n",
    "        idx_list.append(i)\n",
    "        i+=1\n",
    "    idx_list.append(34)\n",
    "    return word2idx_voc, idx_list, idx2word_voc\n",
    "\n",
    "def indexFromPair(pair):\n",
    "    j=1\n",
    "    trg_lis = []\n",
    "    w2ivoc, inp_lis, i2wvoc = indexFromInput(pair[0])\n",
    "    for word in pair[1].split(' '):\n",
    "        try:\n",
    "            trg_lis.append(voc[word])\n",
    "            j=voc[word]\n",
    "        except:\n",
    "            trg_lis.append(j+1)\n",
    "            j+=1\n",
    "    trg_lis.append(34)    \n",
    "    return inp_lis, trg_lis, w2ivoc, i2wvoc\n",
    "        \n",
    "\n",
    "def tensorFromPair(pair):\n",
    "    lisInput,lisTarget,w2ivoc, i2wvoc = indexFromPair(pair)\n",
    "    inp = torch.tensor(lisInput, dtype=torch.long, device=device)\n",
    "    tar = torch.tensor(lisTarget, dtype=torch.long, device=device)\n",
    "    return (inp,tar), w2ivoc, i2wvoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T13:59:15.398012Z",
     "iopub.status.busy": "2021-01-17T13:59:15.397195Z",
     "iopub.status.idle": "2021-01-17T13:59:15.400009Z",
     "shell.execute_reply": "2021-01-17T13:59:15.399470Z"
    },
    "papermill": {
     "duration": 0.034642,
     "end_time": "2021-01-17T13:59:15.400098",
     "exception": false,
     "start_time": "2021-01-17T13:59:15.365456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "# def timeSince(since, percent):\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "#     es = s / (percent)\n",
    "#     rs = es - s\n",
    "#     return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "    return asMinutes(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T13:59:15.458008Z",
     "iopub.status.busy": "2021-01-17T13:59:15.457248Z",
     "iopub.status.idle": "2021-01-17T13:59:15.460139Z",
     "shell.execute_reply": "2021-01-17T13:59:15.459688Z"
    },
    "papermill": {
     "duration": 0.034227,
     "end_time": "2021-01-17T13:59:15.460229",
     "exception": false,
     "start_time": "2021-01-17T13:59:15.426002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T13:59:15.518264Z",
     "iopub.status.busy": "2021-01-17T13:59:15.517526Z",
     "iopub.status.idle": "2021-01-17T13:59:15.519863Z",
     "shell.execute_reply": "2021-01-17T13:59:15.520372Z"
    },
    "papermill": {
     "duration": 0.034102,
     "end_time": "2021-01-17T13:59:15.520475",
     "exception": false,
     "start_time": "2021-01-17T13:59:15.486373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def outVector2wordList(out,vocab):\n",
    "    list1 = []\n",
    "    for i in out.argmax(1):\n",
    "        try:\n",
    "            list1.append(vocab[i.item()])\n",
    "        except:\n",
    "            list1.append('TUNK')\n",
    "    return ' '.join(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T13:59:15.580288Z",
     "iopub.status.busy": "2021-01-17T13:59:15.579401Z",
     "iopub.status.idle": "2021-01-17T13:59:15.582364Z",
     "shell.execute_reply": "2021-01-17T13:59:15.581907Z"
    },
    "papermill": {
     "duration": 0.035641,
     "end_time": "2021-01-17T13:59:15.582460",
     "exception": false,
     "start_time": "2021-01-17T13:59:15.546819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def targetVector2wordsList(trgt,vocab):\n",
    "    list2 = []\n",
    "    for i in range(len(trgt)):\n",
    "        try:\n",
    "            list2.append(vocab[trgt[i].item()])\n",
    "        except:\n",
    "            list2.append('VUNK')\n",
    "    return ' '.join(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T13:59:15.647998Z",
     "iopub.status.busy": "2021-01-17T13:59:15.647339Z",
     "iopub.status.idle": "2021-01-17T13:59:16.430427Z",
     "shell.execute_reply": "2021-01-17T13:59:16.429193Z"
    },
    "papermill": {
     "duration": 0.82176,
     "end_time": "2021-01-17T13:59:16.430566",
     "exception": false,
     "start_time": "2021-01-17T13:59:15.608806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "embedding_size = 300\n",
    "vocab_size = 35\n",
    "epochs = 5\n",
    "patience_s = 100\n",
    "weight_list = [1,1,1,1,1,1,0.9,0.9,0.9,0.9,0.9,0.9,0.7,0.7,0.7,0.7,0.7,0.7,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.1]\n",
    "weight_tensor = torch.tensor(weight_list,dtype=torch.float).to(device)\n",
    "encoder = Encoder(vocab_size, embedding_size, hidden_size).to(device)\n",
    "# encoder.load_state_dict(torch.load(encoder_path,map_location=torch.device('cpu')))\n",
    "decoder = Decoder(vocab_size, embedding_size, hidden_size).to(device)\n",
    "# decoder.load_state_dict(torch.load(decoder_path,map_location=torch.device('cpu')))\n",
    "model = Seq2seq(encoder,decoder).to(device)\n",
    "# model.load_state_dict(torch.load(model_path,map_location=torch.device('cpu')))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "# optimizer = Adam16(model.parameters(), lr=0.0001)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, factor=0.1, verbose=True)\n",
    "# class_weights = torch.tensor(weights,dtype=torch.float16).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weight_tensor).to(device)\n",
    "# training_pairs = [tensorFromPair(pair) for pair in train_pairs[:2200]]\n",
    "# model.eval()\n",
    "# encoder.train()\n",
    "# decoder.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T13:59:16.505374Z",
     "iopub.status.busy": "2021-01-17T13:59:16.504765Z",
     "iopub.status.idle": "2021-01-17T14:42:16.145158Z",
     "shell.execute_reply": "2021-01-17T14:42:16.145911Z"
    },
    "papermill": {
     "duration": 2579.688557,
     "end_time": "2021-01-17T14:42:16.146095",
     "exception": false,
     "start_time": "2021-01-17T13:59:16.457538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 / 5]\n",
      "Train loss: 0.1839,|Valid loss: 0.1071,|Train Jaccard: 0.4604,|Valid Jaccard: 0.4885,|Already gone: 8m 36s\n",
      "train_out: to work tomoo day 1 of 5 it will bee. TUNK EOS\n",
      "train_target: back to work tomoo day 1 of 5 it will bee.\n",
      "valid_out: this flirting going on - the atg smiles. yay. ((hugs)) TUNK TUNK\n",
      "valid_target: all this flirting going on - the atg smiles. yay. ((hugs)\n",
      "[Epoch 2 / 5]\n",
      "Train loss: 0.1098,|Valid loss: 0.1038,|Train Jaccard: 0.4669,|Valid Jaccard: 0.4885,|Already gone: 17m 14s\n",
      "train_out: to work tomoo day 1 of 5 it will bee. TUNK EOS\n",
      "train_target: back to work tomoo day 1 of 5 it will bee.\n",
      "valid_out: this flirting going on - the atg smiles. yay. ((hugs)) TUNK EOS\n",
      "valid_target: all this flirting going on - the atg smiles. yay. ((hugs)\n",
      "[Epoch 3 / 5]\n",
      "Train loss: 0.1080,|Valid loss: 0.1029,|Train Jaccard: 0.4670,|Valid Jaccard: 0.4887,|Already gone: 25m 49s\n",
      "train_out: to work tomoo day 1 of 5 it will bee. TUNK EOS\n",
      "train_target: back to work tomoo day 1 of 5 it will bee.\n",
      "valid_out: this flirting going on - the atg smiles. yay. ((hugs)) TUNK EOS\n",
      "valid_target: all this flirting going on - the atg smiles. yay. ((hugs)\n",
      "[Epoch 4 / 5]\n",
      "Train loss: 0.1072,|Valid loss: 0.1017,|Train Jaccard: 0.4670,|Valid Jaccard: 0.4888,|Already gone: 34m 24s\n",
      "train_out: to work tomoo day 1 of 5 it will bee. TUNK EOS\n",
      "train_target: back to work tomoo day 1 of 5 it will bee.\n",
      "valid_out: this flirting going on - the atg smiles. yay. ((hugs)) TUNK EOS\n",
      "valid_target: all this flirting going on - the atg smiles. yay. ((hugs)\n",
      "[Epoch 5 / 5]\n",
      "Train loss: 0.1067,|Valid loss: 0.1015,|Train Jaccard: 0.4671,|Valid Jaccard: 0.4888,|Already gone: 42m 59s\n",
      "train_out: to work tomoo day 1 of 5 it will bee. TUNK EOS\n",
      "train_target: back to work tomoo day 1 of 5 it will bee.\n",
      "valid_out: this flirting going on - the atg smiles. yay. ((hugs)) TUNK EOS\n",
      "valid_target: all this flirting going on - the atg smiles. yay. ((hugs)\n"
     ]
    }
   ],
   "source": [
    "rubicon = 27000\n",
    "start = time.time()\n",
    "loss1=9999\n",
    "loss2=9999\n",
    "indx = 0\n",
    "for epoch in range(1,epochs+1):\n",
    "    model.train()\n",
    "    print(f\"[Epoch {epoch} / {epochs}]\")\n",
    "    loss_sum = 0\n",
    "    loss_sum_valid = 0\n",
    "    jaccard_score_train = 0\n",
    "    jaccard_score_valid = 0\n",
    "        \n",
    "    for t_pair in train_pairs[:rubicon]:\n",
    "        tp,w2ivocab,i2wvocab = tensorFromPair(t_pair)\n",
    "        inp_data = tp[0]\n",
    "        target = tp[1]\n",
    "        output = model(inp_data, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "        loss_sum +=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        jaccard_score_train += jaccard(outVector2wordList(output[:-1],i2wvocab),t_pair[1])\n",
    "        \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for v_pair in train_pairs[rubicon:]:\n",
    "            vp,w2ivocab_valid,i2wvocab_valid = tensorFromPair(v_pair)\n",
    "            valid_data = vp[0]\n",
    "            valid_target = vp[1]\n",
    "            valid_output = model(valid_data,valid_target)\n",
    "            valid_loss = criterion(valid_output, valid_target)\n",
    "            jaccard_score_valid += jaccard(outVector2wordList(valid_output[:-1],i2wvocab_valid),v_pair[1])\n",
    "            loss_sum_valid +=valid_loss.item()\n",
    "    \n",
    "    \n",
    "            \n",
    "    loss2 = loss1\n",
    "    loss1 = loss_sum_valid/len(train_pairs[rubicon:])\n",
    "    \n",
    "    print(\"Train loss: {:.4f},|Valid loss: {:.4f},|Train Jaccard: {:.4f},|Valid Jaccard: {:.4f},|Already gone: {}\".\n",
    "          format(loss_sum/len(train_pairs[:rubicon]),loss_sum_valid/len(train_pairs[rubicon:]),\n",
    "        jaccard_score_train/len(train_pairs[:rubicon]),jaccard_score_valid/len(train_pairs[rubicon:]),\n",
    "        timeSince(start)\n",
    "    ))\n",
    "    print('train_out:',outVector2wordList(output,i2wvocab))\n",
    "    print('train_target:',t_pair[1])\n",
    "    print('valid_out:',outVector2wordList(valid_output,i2wvocab_valid))\n",
    "    print('valid_target:',v_pair[1])\n",
    "    if loss2<loss1:\n",
    "        indx+=1\n",
    "        torch.save(model.state_dict(),'model_hs1024es300full'+str(epoch)+'epoch.pt')\n",
    "        torch.save(encoder.state_dict(),'encoder_hs1024es300full'+str(epoch)+'epoch.pt')\n",
    "        torch.save(decoder.state_dict(),'decoder_hs1024es300full'+str(epoch)+'epoch.pt')\n",
    "    else:\n",
    "        indx=0\n",
    "    if indx==patience_s:\n",
    "        break\n",
    "    scheduler.step(loss_sum/len(train_pairs[:rubicon]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T14:42:16.238564Z",
     "iopub.status.busy": "2021-01-17T14:42:16.237546Z",
     "iopub.status.idle": "2021-01-17T14:45:51.558946Z",
     "shell.execute_reply": "2021-01-17T14:45:51.559627Z"
    },
    "papermill": {
     "duration": 215.367143,
     "end_time": "2021-01-17T14:45:51.559789",
     "exception": false,
     "start_time": "2021-01-17T14:42:16.192646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46745383725528095\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "jacca = 0\n",
    "for pair in train_pairs:    \n",
    "    pt,w2i,i2w = tensorFromPair(pair)\n",
    "    inp = pt[0]\n",
    "    trg = pt[1]\n",
    "    test_out = model(inp,trg)\n",
    "    jacca += jaccard(pair[1],outVector2wordList(test_out[:-1],i2w))\n",
    "print(jacca/len(train_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T14:45:51.631177Z",
     "iopub.status.busy": "2021-01-17T14:45:51.629366Z",
     "iopub.status.idle": "2021-01-17T14:45:51.631867Z",
     "shell.execute_reply": "2021-01-17T14:45:51.632320Z"
    },
    "papermill": {
     "duration": 0.04163,
     "end_time": "2021-01-17T14:45:51.632437",
     "exception": false,
     "start_time": "2021-01-17T14:45:51.590807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pred(encoder,decoder, source, teacher_force_ratio=0.5, max_length=35):\n",
    "    sourse_len = source.shape[0]\n",
    "    encoder_outputs =  torch.zeros(sourse_len, hidden_size*2, device=device).to(dtype=torch.float16,device=device)\n",
    "    decoder_outputs = []\n",
    "#     for ei in range(sourse_len):\n",
    "    encoder_states, hidden = encoder(source)\n",
    "#         encoder_outputs[ei] = encoder_states\n",
    "\n",
    "#     x = source[0]\n",
    "    x = sos_tensor\n",
    "    for di in range(max_length):\n",
    "        output, hidden = decoder(x, hidden, encoder_states)\n",
    "        best_guess = output.argmax()\n",
    "        \n",
    "        if best_guess==eos_tensor:\n",
    "#             decoder_outputs.append(vocab.index2word[best_guess.item()])\n",
    "            break\n",
    "        else:\n",
    "            decoder_outputs.append(output)\n",
    "\n",
    "    return torch.cat(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T14:45:51.697750Z",
     "iopub.status.busy": "2021-01-17T14:45:51.696887Z",
     "iopub.status.idle": "2021-01-17T14:46:07.127594Z",
     "shell.execute_reply": "2021-01-17T14:46:07.126366Z"
    },
    "papermill": {
     "duration": 15.466004,
     "end_time": "2021-01-17T14:46:07.127801",
     "exception": false,
     "start_time": "2021-01-17T14:45:51.661797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "l = []\n",
    "for sentance in test_list:    \n",
    "    w2i, idx_list, i2w = indexFromInput(sentance)\n",
    "    inp = torch.tensor(idx_list).to(device)\n",
    "    test_out = pred(encoder,decoder,inp)\n",
    "    test_out = outVector2wordList(test_out[:-1],i2w)\n",
    "    l.append(test_out)\n",
    "# print(jacca/len(train_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T14:46:07.195647Z",
     "iopub.status.busy": "2021-01-17T14:46:07.195008Z",
     "iopub.status.idle": "2021-01-17T14:46:07.199414Z",
     "shell.execute_reply": "2021-01-17T14:46:07.198897Z"
    },
    "papermill": {
     "duration": 0.040471,
     "end_time": "2021-01-17T14:46:07.199528",
     "exception": false,
     "start_time": "2021-01-17T14:46:07.159057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "l = pd.Series(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T14:46:07.270452Z",
     "iopub.status.busy": "2021-01-17T14:46:07.268709Z",
     "iopub.status.idle": "2021-01-17T14:46:07.271154Z",
     "shell.execute_reply": "2021-01-17T14:46:07.271627Z"
    },
    "papermill": {
     "duration": 0.040487,
     "end_time": "2021-01-17T14:46:07.271740",
     "exception": false,
     "start_time": "2021-01-17T14:46:07.231253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test['selected_text'] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T14:46:07.340657Z",
     "iopub.status.busy": "2021-01-17T14:46:07.339962Z",
     "iopub.status.idle": "2021-01-17T14:46:07.692107Z",
     "shell.execute_reply": "2021-01-17T14:46:07.691161Z"
    },
    "papermill": {
     "duration": 0.389042,
     "end_time": "2021-01-17T14:46:07.692219",
     "exception": false,
     "start_time": "2021-01-17T14:46:07.303177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "      <td>session of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -...</td>\n",
       "      <td>positive</td>\n",
       "      <td>is also</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eee518ae67</td>\n",
       "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
       "      <td>negative</td>\n",
       "      <td>hit veronique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01082688c6</td>\n",
       "      <td>happy bday!</td>\n",
       "      <td>positive</td>\n",
       "      <td>bday!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33987a8ee5</td>\n",
       "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
       "      <td>positive</td>\n",
       "      <td>- i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3529</th>\n",
       "      <td>e5f0e6ef4b</td>\n",
       "      <td>its at 3 am, im very tired but i can`t sleep  ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>at 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3530</th>\n",
       "      <td>416863ce47</td>\n",
       "      <td>All alone in this old house again.  Thanks for...</td>\n",
       "      <td>positive</td>\n",
       "      <td>alone in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3531</th>\n",
       "      <td>6332da480c</td>\n",
       "      <td>I know what you mean. My little dog is sinkin...</td>\n",
       "      <td>negative</td>\n",
       "      <td>know</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3532</th>\n",
       "      <td>df1baec676</td>\n",
       "      <td>_sutra what is your next youtube video gonna b...</td>\n",
       "      <td>positive</td>\n",
       "      <td>what is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3533</th>\n",
       "      <td>469e15c5a8</td>\n",
       "      <td>http://twitpic.com/4woj2 - omgssh  ang cute n...</td>\n",
       "      <td>positive</td>\n",
       "      <td>- omgssh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3534 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID                                               text sentiment  \\\n",
       "0     f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral   \n",
       "1     96d74cb729   Shanghai is also really exciting (precisely -...  positive   \n",
       "2     eee518ae67  Recession hit Veronique Branquinho, she has to...  negative   \n",
       "3     01082688c6                                        happy bday!  positive   \n",
       "4     33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive   \n",
       "...          ...                                                ...       ...   \n",
       "3529  e5f0e6ef4b  its at 3 am, im very tired but i can`t sleep  ...  negative   \n",
       "3530  416863ce47  All alone in this old house again.  Thanks for...  positive   \n",
       "3531  6332da480c   I know what you mean. My little dog is sinkin...  negative   \n",
       "3532  df1baec676  _sutra what is your next youtube video gonna b...  positive   \n",
       "3533  469e15c5a8   http://twitpic.com/4woj2 - omgssh  ang cute n...  positive   \n",
       "\n",
       "      selected_text  \n",
       "0        session of  \n",
       "1           is also  \n",
       "2     hit veronique  \n",
       "3             bday!  \n",
       "4               - i  \n",
       "...             ...  \n",
       "3529           at 3  \n",
       "3530       alone in  \n",
       "3531           know  \n",
       "3532        what is  \n",
       "3533       - omgssh  \n",
       "\n",
       "[3534 rows x 4 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[['textID','selected_text']].to_csv('submission.csv', index=False)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T14:46:07.760513Z",
     "iopub.status.busy": "2021-01-17T14:46:07.759722Z",
     "iopub.status.idle": "2021-01-17T14:46:07.763990Z",
     "shell.execute_reply": "2021-01-17T14:46:07.763490Z"
    },
    "papermill": {
     "duration": 0.039832,
     "end_time": "2021-01-17T14:46:07.764112",
     "exception": false,
     "start_time": "2021-01-17T14:46:07.724280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# note = 3\n",
    "# pt,w2i,i2w = tensorFromPair(train_pairs[note])\n",
    "# inp = pt[0]\n",
    "# trg = pt[1]\n",
    "# test_out = model(inp,trg)\n",
    "# print(outVector2wordList(test_out,i2w))\n",
    "# print(train_pairs[note])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T14:46:07.833759Z",
     "iopub.status.busy": "2021-01-17T14:46:07.832962Z",
     "iopub.status.idle": "2021-01-17T14:46:07.835327Z",
     "shell.execute_reply": "2021-01-17T14:46:07.835837Z"
    },
    "papermill": {
     "duration": 0.038811,
     "end_time": "2021-01-17T14:46:07.835946",
     "exception": false,
     "start_time": "2021-01-17T14:46:07.797135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# mnb=7\n",
    "# zxc = model(training_pairs[mnb][0],training_pairs[mnb][1])\n",
    "# print('input sentance:',train_pairs[mnb][0])\n",
    "# print('out sentance:',outVector2wordList(zxc))\n",
    "# print('target sentance:',targetVector2wordsList(training_pairs[mnb][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T14:46:07.905682Z",
     "iopub.status.busy": "2021-01-17T14:46:07.904902Z",
     "iopub.status.idle": "2021-01-17T14:46:07.908529Z",
     "shell.execute_reply": "2021-01-17T14:46:07.907866Z"
    },
    "papermill": {
     "duration": 0.040706,
     "end_time": "2021-01-17T14:46:07.908625",
     "exception": false,
     "start_time": "2021-01-17T14:46:07.867919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def softmixer(vec):\n",
    "#     return torch.exp(vec)/sum(torch.exp(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T14:46:07.978805Z",
     "iopub.status.busy": "2021-01-17T14:46:07.977922Z",
     "iopub.status.idle": "2021-01-17T14:46:07.981105Z",
     "shell.execute_reply": "2021-01-17T14:46:07.980644Z"
    },
    "papermill": {
     "duration": 0.039942,
     "end_time": "2021-01-17T14:46:07.981212",
     "exception": false,
     "start_time": "2021-01-17T14:46:07.941270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def evaluation(encoder,decoder, source, teacher_force_ratio=0.5,target_vocab_size = vocab.n_words):\n",
    "#     source = tensorFromIndex(source)\n",
    "#     source_len = source.shape[0]\n",
    "#     encoder_outputs =  torch.zeros(source_len, hidden_size*2, device=device).to(device)\n",
    "#     attention_outputs = []\n",
    "#     decoder_outputs = []\n",
    "#     for ei in range(source_len):\n",
    "#         encoder_states, hidden = encoder(source[ei])\n",
    "#         encoder_outputs[ei] = encoder_states\n",
    "\n",
    "# #     x = source[0]\n",
    "#     x = sos_tensor\n",
    "#     for di in range(source_len):\n",
    "#         output, hidden, attention = decoder(x, hidden, encoder_outputs)\n",
    "#         attention_outputs.append(attention.cpu().squeeze(1).detach().numpy())\n",
    "#         best_guess = output.argmax()\n",
    "#         x = best_guess\n",
    "#         if best_guess.item()==eos_tensor.item():\n",
    "# #             decoder_outputs.append(vocab.index2word[best_guess.item()])\n",
    "#             break\n",
    "#         else:\n",
    "#             decoder_outputs.append(vocab.index2word[best_guess.item()])\n",
    "        \n",
    "#         max_tensor,_ = torch.tensor(attention_outputs).max(axis=0)\n",
    "#         softmax_tensor = softmixer(max_tensor)\n",
    "        \n",
    "#     return decoder_outputs, softmax_tensor, torch.tensor(attention_outputs),hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T14:46:08.052055Z",
     "iopub.status.busy": "2021-01-17T14:46:08.051310Z",
     "iopub.status.idle": "2021-01-17T14:46:08.053839Z",
     "shell.execute_reply": "2021-01-17T14:46:08.054514Z"
    },
    "papermill": {
     "duration": 0.0399,
     "end_time": "2021-01-17T14:46:08.054634",
     "exception": false,
     "start_time": "2021-01-17T14:46:08.014734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# change = 23\n",
    "# print(test_list[change])\n",
    "# out,smt, attn,h = evaluation(encoder,decoder,test_list[change])\n",
    "# print(out)\n",
    "# print(smt)\n",
    "# print(attn)\n",
    "# print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T14:46:08.126798Z",
     "iopub.status.busy": "2021-01-17T14:46:08.124989Z",
     "iopub.status.idle": "2021-01-17T14:46:08.127554Z",
     "shell.execute_reply": "2021-01-17T14:46:08.128069Z"
    },
    "papermill": {
     "duration": 0.040117,
     "end_time": "2021-01-17T14:46:08.128183",
     "exception": false,
     "start_time": "2021-01-17T14:46:08.088066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# change = 2\n",
    "# print(pairs_list[change][0])\n",
    "# out,smt, attn,h = evaluation(encoder,decoder,pairs_list[change][0])\n",
    "# print(out)\n",
    "# print(smt)\n",
    "# print(attn)\n",
    "# print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T14:46:08.197110Z",
     "iopub.status.busy": "2021-01-17T14:46:08.196510Z",
     "iopub.status.idle": "2021-01-17T14:46:08.200419Z",
     "shell.execute_reply": "2021-01-17T14:46:08.199876Z"
    },
    "papermill": {
     "duration": 0.039954,
     "end_time": "2021-01-17T14:46:08.200543",
     "exception": false,
     "start_time": "2021-01-17T14:46:08.160589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# change = 2\n",
    "# print(pairs_list[change][0])\n",
    "# out,smt, attn,h = evaluation(encoder,decoder,pairs_list[change][0])\n",
    "# print(out)\n",
    "# print(smt)\n",
    "# print(attn)\n",
    "# print(h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 2828.526539,
   "end_time": "2021-01-17T14:46:08.745481",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-17T13:59:00.218942",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
